---
layout: post
title: "optuna"
date: 2020-12-27
author: "@vsoch"
annotate_criteria: https://rseng.github.io/software/repository/github/optuna/optuna/annotate-criteria/index.html
annotate_taxonomy: https://rseng.github.io/software/repository/github/optuna/optuna/annotate-taxonomy/index.html
categories:
- Software
---


Hyperparameter tuning has been around as long as any major cornerstone of machine learning. So why another package for tuning hyperparameters?
This week on the Research Software Showcase we share [optuna](https://github.com/optuna/optuna), which we assure you is 
worth trying out if you need to do this kind of optimization.

<br>

![{{ site.baseurl }}/assets/img/posts/showcase/optuna.png]({{ site.baseurl }}/assets/img/posts/showcase/optuna.png)

<br>

Are you already familiar with this software? We encourage you to contribute to the [research software encyclopedia](https://rseng.github.io/rse/tutorials/annotation/) and annotate the respository:

<ul>
<li><a href="{{ page.annotate_criteria }}" target="_blank">Annotate the software criteria</a></li>
<li><a href="{{ page.annotate_taxonomy }}" target="_blank">Annotate the software taxonomy</a></li>
</ul>

otherwise, keep reading!

<!--more--> 

 - [What is optuna?](#what-is)
 - [How do I cite it?](#cite)
 - [How do I get started?](#getting-started)
 - [How do I contribute to the software survey](#contribute)
 - [Where can I learn more?](#learn-more)

<a id="what-is">
## What is optuna?

Tuning hyperparameters can often be pretty difficult. Essentially, it is an optimization problem -- but in the most general setting there are no gradients to use, and problems can be fairly high-dimensional. For this reason many hyperparameter tuning libraries have focused on easily parallelizing the parameter search, while others have added pruning abilities (stop unpromising trials early to save time).

The main selling point of Optuna is allowing the user to dynamically construct their search space using a 'define-by-run' paradigm. This approach has been increasingly used for training neural networks (by allowing the user to dynamically define a computational graph) but until now has not been used for hyperparameter tuning. In their paper, the authors include a snippet of code that defines a search space dynamically versus statically. In order to achieve the same behavior with a statistically-defined search space, the code becomes extremely convoluted even in a simple example.

Optuna is also lightweight and easy to set up, efficient, parallelizable, and comes with a nice set of visualization tools. It is also highly modular because of the define-by-run design. Here is a quick example from their documentation to:

 1. Define an objective function to be optimized (in the example, we minimize `(x - 2)^2`
 2. Suggest a range of hyperparameter values (e.g., -10 to 10)
 3. Create a `study` object and optimize over 100 trials

```python
import optuna

def objective(trial):
    x = trial.suggest_uniform('x', -10, 10)
    return (x - 2) ** 2

study = optuna.create_study()
study.optimize(objective, n_trials=100)

study.best_params  # E.g. {'x': 2.002108042}
```

This is really intuitive and simple, which makes it great! You should check out the [code examples](https://optuna.org/#code_examples) that show interaction with
additional libraries like PyTorch, Tensorflow, Sci-kit learn, and others.

<a id="cite">
## How do I cite it?

You can reference their pre-print paper on [ArXiv](https://arxiv.org/abs/1907.10902) or from proceedings (preferred):

```
@inproceedings{optuna_2019,
    title={Optuna: A Next-generation Hyperparameter Optimization Framework},
    author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
    booktitle={Proceedings of the 25rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
    year={2019}
}

@misc{akiba2019optuna,
      title={Optuna: A Next-generation Hyperparameter Optimization Framework}, 
      author={Takuya Akiba and Shotaro Sano and Toshihiko Yanase and Takeru Ohta and Masanori Koyama},
      year={2019},
      eprint={1907.10902},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

<a id="getting-started">
## How do I get started?
 
The main site documentation is really great because it provides many different methods
of learning, from reading, to examples, to video.

 - [Basic Concepts and Colab Tutorial](https://github.com/optuna/optuna#basic-concepts)
 - [Tutorials](https://optuna.readthedocs.io/en/stable/tutorial/index.html)
 - [Key Features](https://github.com/optuna/optuna#key-features)
 - [Code Examples](https://optuna.org/#code_examples)
 - [Videos](https://optuna.org/#video)

<a id="contribute">
## How do I contribute to the software survey?

<ul>
  <li><a href="{{ page.annotate_criteria }}" target="_blank">Annotate the software criteria</a></li>
  <li><a href="{{ page.annotate_taxonomy }}" target="_blank">Annotate the software taxonomy</a></li>
</ul>

or read more about annotation [here]({{ site.baseurl }}/tutorials/annotate-your-software). You can clone the software repository to do
bulk annotation, or annotation any repository in the <a href="https://rseng.github.io/software/" target="_blank">software database</a>,
We want annotation to be fun, straight-forward, and easy, so we will be showcasing one repository to annotate per week.
If you'd like to request annotation of a particular repository (or addition to the software database)
please don't hesitate to [open an issue](https://github.com/rseng/software/issues) or even a pull request.

<a id="learn-more">
## Where can I learn more?

You might find these other resources useful:

 - [The Research Software Database](https://github.com/rseng/software) on GitHub
 - [RSEpedia Documentation](https://rseng.github.io/rse)
 - [Google Docs Manuscript](https://docs.google.com/document/d/1wDb0udH9OrFWrMBsAVb8RrUMCKKRHoyEep7yveJ1d0k/edit) you are invited to contribute to.
 - [Annotation Documentation for RSEpedia](https://rseng.github.io/rse/tutorials/annotation/)
 - [Annotation Tutorial in RSEng docs](https://rseng.github.io/rse/tutorials/annotation/)

For any resource, you are encouraged to give feedback and contribute!
