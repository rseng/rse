---
layout: post
title: "autograd"
date: 2020-10-18
author: "@tabakg"
annotate_criteria: https://rseng.github.io/software/repository/github/HIPS/autograd/annotate-criteria/
annotate_taxonomy: https://rseng.github.io/software/repository/github/HIPS/autograd/annotate-taxonomy/
categories:
- Software
---

Have you ever wanted to use automatic differentiation but your code is already written in numpy? Or just don't want to learn another library? Today we are showcasing
 <a href="https://github.com/HIPS/autograd" target="_blank">HIPS/autograd</a>, which will allow you to do just that. That's right, just write a function in numpy, and get all of its gradients for free.


<br>

![{{ site.baseurl }}/assets/img/posts/showcase/tanh.png]({{ site.baseurl }}/assets/img/posts/showcase/tanh.png)

<br>

We encourage you to contribute to the [research software encyclopedia](https://rseng.github.io/rse/tutorials/annotation/) and annotate the respository:

<ul>
<li><a href="{{ page.annotate_criteria }}" target="_blank">Annotate the software criteria</a></li>
<li><a href="{{ page.annotate_taxonomy }}" target="_blank">Annotate the software taxonomy</a></li>
</ul>

<!--more--> 

 - [What is statsmodels?](#what-is)
 - [How do I cite it?](#cite)
 - [How do I contribute to the software survey](#contribute)
 - [How do I get started](#getting-started)
 - [Where can I learn more?](#learn-more)

<a id="what-is">
## What is Autograd?

### What do you mean get gradients for free?

Autograd has a thin wrapper for numpy. you'd just import it instead of the usual numpy, and not have to change any of your actual numpy code. From their README:

```python
>>> import autograd.numpy as np  # Thinly-wrapped numpy
>>> from autograd import grad    # The only autograd function you may ever need
```

That's it! We are now ready to differentiate functions.

```python
>>> f = lambda x: np.sin(x*2) / x
>>> grad(f)(1.0)
-1.7415910999199666
```

You can apply it as many times as you want.

### But can I ...

Autograd has a rich set of features. It supports multi-variable differentiation (so it can be used for backpropagation), and even control flow operations including branches, recursion, loops, and clojures. Here are some examples I tried:

Control flow:

```python
>>> def f(x):
... if x < 0.0:
...     return f(-x)
... while x > 10:
...     x /= 10
... return np.power(x, 1.5)
... 
>>> grad(f)(-10003.0)                                                      
-0.00015002249831275308
```

Jacobian:

```python
>>> from autograd import jacobian

>>> f = lambda x: np.array(
... [2 * x[0]**2 + 3 * x[1], np.cos(x[0]) + np.sin(x[1])])
>>> jacobian(f)(np.array([2.0, 3.0]))
array([[ 8.        ,  3.        ],
       [-0.90929743, -0.9899925 ]])
```

Complex-valued function of multiple variables:

```python
>>> from autograd import holomorphic_grad
>>> f = lambda x: x[0]*2 + x[1]
>>> holomorphic_grad(f)(np.array([6.0j, 1.0j]))
array([2.+0.j, 1.+0.j])
```

### But does it scale?!

That's the milion dollar question these days. Luckily for us, the vision of Autograd is being carried on in the high-performance context with [JAX](https://github.com/google/jax), which utilizes XLA (Accelerated Linear Algebra), originally developed for Tensorflow. Although this is still a young library, it promises a lot by combining the best of both worlds.


<a id="cite">
## How do I cite it?

You can cite autograd as follows, and a pdf of the paper is available <a href="https://indico.ijclab.in2p3.fr/event/2914/contributions/6483/subcontributions/180/attachments/6060/7185/automl-short.pdf" target="_blank">here</a>:

```
@inproceedings{maclaurin2015autograd,
  title={Autograd: Effortless gradients in numpy},
  author={Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P},
  booktitle={ICML 2015 AutoML Workshop},
  volume={238},
  pages={5},
  year={2015}
}
```

<a id="getting-started">
## How do I get started?
 
 - [Autograd Documentation](https://github.com/HIPS/autograd/blob/master/docs/tutorial.md) has a nice tutorial of implementing logistic regression from start to finish, and lots of other useful information.

<a id="contribute">
## How do I contribute to the software survey?

<ul>
  <li><a href="{{ page.annotate_criteria }}" target="_blank">Annotate the software criteria</a></li>
  <li><a href="{{ page.annotate_taxonomy }}" target="_blank">Annotate the software taxonomy</a></li>
</ul>

or read more about annotation [here]({{ site.baseurl }}/tutorials/annotate-your-software). You can clone the software repository to do
bulk annotation, or annotation any repository in the <a href="https://rseng.github.io/software/" target="_blank">software database</a>,
We want annotation to be fun, straight-forward, and easy, so we will be showcasing one repository to annotate per week.
If you'd like to request annotation of a particular repository (or addition to the software database)
please don't hesitate to [open an issue](https://github.com/rseng/software/issues) or even a pull request.

<a id="learn-more">
## Where can I learn more?

You might find these other resources useful:

 - [The Research Software Database](https://github.com/rseng/software) on GitHub
 - [RSEpedia Documentation](https://rseng.github.io/rse)
 - [Google Docs Manuscript](https://docs.google.com/document/d/1wDb0udH9OrFWrMBsAVb8RrUMCKKRHoyEep7yveJ1d0k/edit) you are invited to contribute to.
 - [Annotation Documentation for RSEpedia](https://rseng.github.io/rse/tutorials/annotation/)
 - [Annotation Tutorial in RSEng docs](https://rseng.github.io/rse/tutorials/annotation/)

For any resource, you are encouraged to give feedback and contribute!
